%-------------------------------------------------------------------------------
% IPOL article about Non-local Sparse Models for Image Restoration
% by Yohann Salaun &Marc Lebrun
% February 2013
%-------------------------------------------------------------------------------
\documentclass{ipol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage{algorithm2e}
\usepackage{bm}
\usepackage{verbatim}

% commands for dictionary/patches...
\newcommand{\psize}{m}   
\newcommand{\dsize}{k}  
\newcommand{\dict}{\textbf{D}}  
\newcommand{\code}{\bm{\alpha}}  
\newcommand{\pnorm}{p}
% commands for noisy/denoised picture
\newcommand{\Denoi}{\textbf{X}}  
\newcommand{\denoi}{\textbf{x}}
\newcommand{\Noi}{\textbf{Y}}
\newcommand{\noi}{\textbf{y}}
% commands for LARS
\newcommand{\mua}{\bm{\mu}_{\mathcal{A}}}
\newcommand{\da}{\bm{D}_{\mathcal{A}}}
\newcommand{\Aa}{A_{\mathcal{A}}}
\newcommand{\una}{\mathds{1}_{\mathcal{A}}}
\newcommand{\ua}{\bm{u}_{\mathcal{A}}}
\newcommand{\clars}{\hat{\textbf{c}}}
\newcommand{\A}{\mathcal{A}}
% usual commands
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\argmin}{\text{argmin}}

\ipolSetTitle{LSSC}
\ipolSetAuthors{Yohann Salaun$^1$ \&Marc Lebrun$^2$}
\ipolSetAffiliations{%
$^1$ Polytechnique, France (\texttt{yohann.salaun@polytechnique.org})\\
$^2$ CMLA, ENS Cachan, France (\texttt{marc.lebrun@cmla.ens-cachan.fr})}

\begin{document}

%-------------------------------------------------------------------------------
\begin{abstract}

\end{abstract} 

%-------------------------------------------------------------------------------
\section{Overview}


%-------------------------------------------------------------------------------
\section{Theoritical Description}

\subsection{ Notations}

In order to keep coherence with \cite{LSSC}, the notations used are the same.
A picture of $n$ pixels is seen as a column vector in $\Rn$. The noisy picture is noted $\noi$ and the denoised one $\denoi$. The $i$-th pixel of x is noted $\denoi[i]$ and the patch centered in $\denoi[i]$ and of size $\psize$ is noted $\denoi_i$.

\subsection{ Learned Sparse Coding}

The idea behind this method is to assume that the denoised picture is a signal that can be approximated by a sparse linear combinations of elements from a basis set. The basis set is called a dictionary $\dict \in \RR^{\psize \times \dsize}$ and is composed of $\dsize$ elements. The denoised patches are then computed from $\dict$ with:\\
\begin{equation}
	min_{\code\in \RR^\dsize} ||\code||_\pnorm \ \ s.t. \ \ ||\noi_i  - \dict \code||^2_2 \leqslant \epsilon
	\label{sparseDict}
\end{equation}
$\dict \code$ is the estimate of the denoised patch and $||\code||_\pnorm $ is a regularization term that impose sparsity for $\code$.\\ 
$\pnorm$ is usually 0 or 1. Eq. \ref{sparseDict} becomes NP-hard to solve when $\pnorm=0$ and a greedy algorithm such as Orthogonal Matching Pursuit \cite{OMP} can give an approximation. With $\pnorm=1$, the problem is convex and solved efficiently with the LARS algorithm \cite{LARS}. Experimental observations \cite{l0l1} have shown that the learning part is better with $\pnorm=1$ and the recomposition part with $\pnorm=0$.\\
 $\epsilon$ can be chosen according to the value of the estimated standard deviation of the noise.

\subsection {Simultaneous Sparse Coding}


%-------------------------------------------------------------------------------
\section{Algorithm Description}

\subsection{ Algorithm Overview }

The first part consists in initializing a dictionary that will denoise roughly the picture.\\
Once the picture is denoised a first time, a clustering is made in order to regroup similar patches for further treatment.\\
Then, iteratively for each clusters, the dictionary is updated using simultaneous sparse coding and the cluster is denoised.

\subsection{Dictionnary Initialization}

The initial dictionnary is first learned offline on the 10 000 images of the PASCAL VOC'07 database using the  online dictionary learning procedure of \cite{onlineLearning}. This procedure is then used on the noisy picture in order to improve the dictionary efficiency.\\
In fact, only a fixed number $T$ of patches in the picture are used to update the dictionary. However they are chosen so that they are independently and identically distributed in the picture.\\
The algorithm corresponds then to the minimization of eq.\ref{sparseDict} on the $T$ patches with the $l_1$ norm using the LARS \cite{LARS} algorithm.\\

\begin{algorithm}[H]

\SetLine
\textbf{Input} : number of iterations \textbf{T},  i.i.d. sampling of \textbf{T} patches of the noisy picture \textbf{Y}$_T$, initial dictionary \dict$^0$ $\in \RR^{\psize \times \dsize}$, regularization parameter $\lambda$ \\
\textbf{Output} : learned dictionary \dict \\
\textbf{Initialization} : \textbf{A}$^0 \in \RR^{\dsize \times \dsize} \leftarrow 0$,  \textbf{B}$^0 \in \RR^{\psize \times \dsize} \leftarrow 0$ \\
	\For{ t = 1..\textbf{T}}{
		\textbf{\noi}$_t$ = \textbf{\Noi}$_T$[t]\\
		Sparse coding: compute with LARS algorithm:
\[
		\textbf{$\code$}^t = \argmin_{\code \in \RR^\dsize} ||\code||_1 \text{ s.t. } ||\noi - \dict^{t-1} \code||_2^2 \leq \lambda
\]
		\textbf{A}$^t \leftarrow \textbf{A}^{t-1} + \code^t \code^{t T}$\\
		\textbf{B}$^t \leftarrow \textbf{B}^{t-1} + \textbf{\noi}^t \code^{t T}$\\
		Update dictionary from \dict$^{t-1}$ to \dict$^{t}$ so that:
\[
		\dict^{t}= \argmin_{\dict \in\RR^{\psize \times \dsize}} \frac{1}{t} \sum_{i=1}^t (\frac{1}{2}|\textbf{\noi}^i - \dict \code^i||^2_2 + \lambda ||\code^i||_1)
\]
}
\Return \dict$^{T}$

\caption{Online Dictionary Learning}

\end{algorithm}

\begin{algorithm}[H]

\SetLine
\textbf{Input} :input dictionary \dict = [\textbf{d}$^1$, ... , \textbf{d}$^\dsize$] $\in \mathbb{R}^{\psize \times \dsize}$, \\ \textbf{A} = [\textbf{a}$^1$, ... , \textbf{a}$^\dsize$] $ \in \mathbb{R}^{\dsize \times \dsize}$,  \textbf{B} = [\textbf{b}$^1$, ... , \textbf{b}$^\dsize$]$ \in \mathbb{R}^{\psize \times \dsize}$\\
\textbf{Output} : updated dictionary \textbf{\dict} \\
\Repeat{convergence}{
	\For{ j = 1..\dsize}{
		update the $j^{th}$ column:
\[
		\textbf{u}^{j}\leftarrow \frac{1}{\textbf{A}(j,j)} (\textbf{b}^{j} -\dict\textbf{a}^{j}) + \textbf{d}^{j}
\]
\[
		\textbf{d}^{j}\leftarrow \frac{1}{\max(||\textbf{u}^{j}||_2, 1)}\textbf{u}^{j}
\]
	}
}

\Return \dict

\caption{Dictionary Update}

\end{algorithm}

\newpage

\begin{algorithm}[H]

\SetLine
\textbf{Input} : Gram matrix of the dictionary G = $\dict^T\dict \in \mathbb{R}^{\dsize \times \dsize}$ where $\dict$ = [\textbf{d}$^1$, ... , \textbf{d}$^\dsize$] $\in \mathbb{R}^{\psize \times \dsize}$,\\
noisy patch \textbf{\noi}, 
constraint $\lambda$\\
\textbf{Output} : sparse vector $\code \in \mathbb{R}^{\dsize}$\\
---INITIALIZATION---\\
normX $\in \mathbb{R} \leftarrow$ $||\noi||^2_2$\\
coeffs $\in \mathbb{R}^{\dsize} \leftarrow$ \textbf{0}\\
\textit{Most correlated element}\\
$\clars \in \RR^{\dsize} \leftarrow \dict^T \noi$\\
$C \in \RR^+ \leftarrow \max_{j=1..\psize}(|\clars_j|)$ \\
currentInd $\leftarrow j \text{ s.t. } \clars_j = C$\\
\If{ normX $> \lambda$}{
	\textbf{return 0}
}
newAtom $\leftarrow$ \textbf{True}\\
\For{ i = 1..$\dsize$}{
	---NEW ATOM---\\
	\If{ newAtom }{
		$\A$[i] $\leftarrow$ currentInd\\
		$G_A$[$i^{th}$ line] $\leftarrow$ $G$[$\A$[i]$^{th}$ line]\\
		$G_S$ $\leftarrow$ $\da^T\da$\\
		UPDATE $G_S^{-1}$
	}
	---VARIABLES UPDATES---\\
	u $\leftarrow$ $G_S^{-1} (\text{sgn}(\clars_j))_{j\in \A}$\\
	ratio $\leftarrow$ $\left(-\frac{\text{coeffs}[j]}{u_j}\right)_{j\in[1;i]}$\\
	stepMAX $\leftarrow$ $\min^+(\text{ratio})$ ($\min^+$ means its the minimum between positive values)\\
	criticalInd $\leftarrow$ j s.t. ratio[j] =	stepMAX\\
	C $\leftarrow$ $\clars[0]$\\
	$\gamma$ $\leftarrow$ $\min^+\left(\frac{C \pm \clars_j}{1 \pm (G_Au)[j]}\right)_{j \notin \A}$\\
	---POLYNOMIAL RESOLUTION---\\
	$a$ $\leftarrow$ $\sum_{j \in \A} \text{sgn}(\clars[\A[j]])u[j]$\\
	$b$ $\leftarrow$ $\sum_{j \in \A} \clars[\A[j]] u[j]$\\
	$c$ $\leftarrow$ normX - $\lambda$\\
	$\Delta$ $\leftarrow$ $b^2 - ac$ \\
	stepMAX2 $\leftarrow$ $\min(\frac{b - \sqrt{\Delta}}{a}, C)$ \\
	---FINAL STEP $\&$ BREAK---\\
	$\gamma$ $\leftarrow$ min($\gamma$, stepMAX, stepMAX2)\\
	coeffs $\leftarrow$ coeffs + $\gamma$u \\
	$\clars$ $\leftarrow$ $\clars - \gamma G_Au$ \\
	normX $\leftarrow$ normX + $a\gamma^2 - 2b\gamma$ \\
	\If{ $|\gamma| < 1e^{-15} \ ||\ \gamma = stepMAX2 \ ||\ normX < 1e^{-15} \ ||\ normX - \lambda < 1e^{-15}$}{
		\textbf{break}
	}
	\If{ $\gamma = stepMAX$ }{
		DOWNDATE $G_S^{-1}$ \\
		newAtom $\leftarrow$ \textbf{False} \\
		i $\leftarrow$ i-2
	}
	\Else{
		newAtom $\leftarrow$ \textbf{True}
	}
}
\Return $\code = sort($coeffs$, \A)$ (sorted coefficient with respect to the increasing order of $\A$ indexes and filled with 0 if necessary)
	
\caption{LARS algorithm - Mairal Version}

\end{algorithm}

\newpage

\begin{algorithm}[H]

\SetLine
\textbf{Input} : Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its former inverse to update $G_S^{-1}\in \mathbb{R}^{i-1 \times i-1}$\\
\textbf{Output} : updated  $G_S^{-1}\in \mathbb{R}^{i \times i}$\\
\If{i = 1}{
	\Return $\frac{1}{G_S}$
} 
$u \ \leftarrow \ G_S^{-1} G_S[i^{th}$ line]\\
$\sigma$ $\leftarrow \ \frac{1}{G_s(i,i) - u.G_S[i^{th} \text{ line}]}$\\
$G_s^{-1}(i,i)\ \leftarrow$ $\sigma$\\
$G_s^{-1}[i^{th}$ line] $\leftarrow$ -$\sigma u$\\
\Return $G_s^{-1} \ \leftarrow \ G_s^{-1}+\sigma u u^T$\\
\caption{Update invert algorithm}

\end{algorithm}

\begin{algorithm}[H]

\SetLine
\textbf{Input} : pseudo-Gram matrix $G_A \in \mathbb{R}^{i \times i}$
Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its 
inverse $G_S^{-1}\in \mathbb{R}^{i \times i}$, active indexes list $\A \in \mathbb{R}^i$, sparse coefficient list coeffs $\in \mathbb{R}^{\dsize}$, criticalInd $\in [1; \dsize]$, current iteration $i$\\
\textbf{Output} : downdated matrices $G_S, G_S^{-1}, G_A\in \mathbb{R}^{i-1 \times i-1}$, downdated lists $\A \in \mathbb{R}^{i-1}$, coeffs $\in \mathbb{R}^{\dsize}$\\

$\sigma \ \leftarrow \ \frac{1}{G_S^{-1}(\text{criticalInd}, \text{criticalInd})}$\\
$u \ \leftarrow \ G_S^{-1} [$criticalInd$^{th}$ line] without its criticalInd$^{th}$ coefficient\\
\For{j= criticalInd:i}{
	$G_A[j^{th}$ line] $\leftarrow$ $G_A[(j+1)^{th}$ line]\\
	\For{k= 1:criticalInd-1}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k)$\\
	}
	\For{k= criticalInd:i}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k+1)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k+1)$\\
	}
	$\A[j] \ \leftarrow \ \A[j+1]$\\
	coeffs[j] $\leftarrow$ coeffs[j+1]\\
}
coeffs[i] $\leftarrow$ 0\\
$G_s^{-1} \ \leftarrow \ G_s^{-1}-\sigma u u^T$\\

\caption{Downdate invert algorithm}

\end{algorithm}

%-------------------------------------------------------------------------------
\section*{Glossary}



%-------------------------------------------------------------------------------
\section*{Image Credits}

\includegraphics[height=2em]{ipol_logo} \copyright\ IPOL (there's no
need to credit this image, here is used as an example.)

%-------------------------------------------------------------------------------
\section{References}

\begin{thebibliography}{9}

\bibitem{LARS}
	B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani
	\emph{Least angle regression}. 
	Ann. Statist.,
	32(2):407–499,
	2004.

\bibitem{onlineLearning}
	J.Mairal, F. Bach, J. Ponce, and G. Sapiro
	\emph{Online dictionary learning for sparse coding}.
	ICML,
	2009.

\bibitem{LSSC}
	J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman
	\emph{ Non-local Sparse Models for Image Restoration}.
	International Conference on Computer Vision,
	2009.

\bibitem{thesis}
	J. Mairal
	\emph{ Représentations parcimonieuses en apprentissage statistique, traitement d’image et
vision par ordinateur}.
	PhD thesis,
	2010.

\bibitem{OMP}
	S. Mallat and Z. Zhang
 	\emph{Matching pursuit in a timefrequency dictionary}.
	IEEE T. SP,
	41(12):3397–3415,
	1993.

\bibitem{LARS}
	B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani
	\emph{ Least angle regression}.
	Ann. Statist.,
	32(2):407–499,
	2004.

\bibitem{l0l1}
	M. Elad and M. Aharon
	\emph{Image denoising via sparse and redundant representations over learned dictionaries}.
	IEEE T.
	IP, 54(12):3736–3745,
	2006.

\end{thebibliography}

\begin{comment}

\begin{algorithm}[H]

\SetLine
\textbf{Input} : dictionary \dict = [\textbf{d}$^1$, ... , \textbf{d}$^\dsize$] $\in \mathbb{R}^{\psize \times \dsize}$, noisy patch \textbf{\noi}, regularization parameter $\lambda$\\
\textbf{Output} : minimal argument $\code \in \RR^k$\\
\textbf{Initialization} : $\mua \in \RR^k \leftarrow 0$, $\code \in \RR^k \leftarrow 0$\\
	\For{ i = 1..\psize}{
		---INITIALIZATION---\\
		\If{ first iteration }{
		\textit{Most correlated element}\\
		$\clars \in \RR^{\dsize} \leftarrow \dict^T (\noi - \mua)$\\
		\textit{Highest correlation}\\
		$C \in \RR^+ \leftarrow \max_{j=1..\psize}(|\clars_j|)$ \\
		\textit{Active index set}\\
		$\A \leftarrow \{ j \text{ s.t. } |\clars_j| = C\}$\\
	}
		---VARIABLES UPDATES---\\
		\textit{The inversion of $\da^T\da$ is done iteratively either with Cholesky decomposition or inversion lemma.}\\
		$\da \in \RR^{\psize \times |\A|}\leftarrow \left( ... s_j\textbf{d}_j ...\right)$ where $s_j = \text{sgn}((\dict^T (\noi - \mua))_j)$\\
		$\ua \in \RR^{|\A|} \leftarrow \Aa\da\left(\da^T\da\right)^{-1}\una$ where $\Aa = \frac{1}{\sqrt{\una^T \left(\da^T\da\right)^{-1}\una}}$ and $\una^T = (1,...,1) \in \RR^{|\A|}$\\
		$a \in \RR^{\psize} \leftarrow \dict^T\ua$\\
		---NEW ACTIVE INDEX---\\
		\textit{$\min^+$ means that it only concerns positive values}\\
		$\gamma \leftarrow \min^+_{j \in \A^C} \left[ \frac{C - c_j}{\Aa - a_j},  \frac{C + c_j}{\Aa + a_j} \right]$\\
		$\mua \leftarrow \mua + \gamma \ua$ \\
		\textit{New highest correlation}\\
		$C \in \RR^+ \leftarrow C - \gamma \Aa$ \\
		\textit{New active index set}\\
		$\A \leftarrow \A \cup \{ j\text{ s.t. } \gamma_j = \gamma \}$\\
	}
	
\caption{LARS algorithm}

\end{algorithm}

\end{comment}

\end{document}
%-------------------------------------------------------------------------------
