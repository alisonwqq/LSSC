%-------------------------------------------------------------------------------
% IPOL article about Non-local Sparse Models for Image Restoration
% by Yohann Salaun & Marc Lebrun
% February 2013
%-------------------------------------------------------------------------------
\documentclass{ipol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm2e}
\usepackage{bm}
\usepackage{verbatim}
\usepackage[hang,small]{caption}

% commands for dictionary/patches...
\newcommand{\psize}{m}   
\newcommand{\dsize}{k}  
\newcommand{\dict}{\textbf{D}}  
\newcommand{\code}{\bm{\alpha}}  
\newcommand{\pnorm}{p}
\newcommand{\qnorm}{q}
\newcommand{\nclust}{n_C}
% commands for noisy/denoised picture
\newcommand{\Denoi}{\textbf{X}}
\newcommand{\denoi}{\textbf{x}}
\newcommand{\Noi}{\textbf{Y}}
\newcommand{\noi}{\textbf{y}}
% commands for LARS
\newcommand{\mua}{\bm{\mu}_{\mathcal{A}}}
\newcommand{\da}{\bm{D}_{\mathcal{A}}}
\newcommand{\Aa}{A_{\mathcal{A}}}
\newcommand{\una}{\mathds{1}_{\mathcal{A}}}
\newcommand{\ua}{\bm{u}_{\mathcal{A}}}
\newcommand{\clars}{\hat{\textbf{c}}}
\newcommand{\A}{\mathcal{A}}
% usual commands
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\argmin}{\text{argmin}}

\ipolSetTitle{LSSC}
\ipolSetAuthors{Yohann Salaun$^1$ \& Marc Lebrun$^2$}
\ipolSetAffiliations{%
$^1$ Polytechnique, France (\texttt{yohann.salaun@polytechnique.org})\\
$^2$ CMLA, ENS Cachan, France (\texttt{marc.lebrun@cmla.ens-cachan.fr})}

\begin{document}

%-------------------------------------------------------------------------------
\begin{abstract}

\end{abstract} 

%-------------------------------------------------------------------------------
\section{Overview}


%-------------------------------------------------------------------------------
\section{Theoretical Description}

\subsection{ Non Local Means }

Self similarities inside pictures have been studied for a long time. Efros and Leung used it for texture synthesis \cite{Patch01} and were followed by Buades, Coll and Morel for denoising purpose \cite{Patch02}.\\
Considering a noisy picture $\noi$ of $n$ pixels seen as a column vector in $\Rn$, $\noi$ is divided into $n$ overlapping patches of equal size $\psize$. The $i$-th pixel of $\noi$ is noted $\noi[i]$ and the patch centered in $\noi[i]$ and of size $\psize$ is noted $\noi_i$. Then, when two different patches $\noi_i$ and $\noi_j$ have similar values, their denoised version should also be similar. Moreover, assuming that the noise follows a Gaussian distribution, averaging similar patches should destroy the noise information and thus denoise the patch. Thus the denoised pixel $\denoi[i]$ is obtained with a linear combination of the others pixel in the noisy picture $\noi$ weighted by the similarity of their corresponding patches:
\begin{equation}
	\denoi[i] = \frac{1}{N} \sum_{j=1}^n G(\noi_i - \noi_j) \noi[j]
\end{equation}
where $N$ is the normalization factor and $G$ a Gaussian that weights the patch similarities.\\ 
Following this idea, Mairal et al. \cite{LSSC} divided the pictures into $n$ overlapping patches in order to denoise through self-similarity methods.

\subsection{ Learned Sparse Coding}

The second idea is to assume that the denoised patches can be approximated by a sparse linear combinations of elements from a basis set.\\
This basis set, that contains the denoised picture information, is called a dictionary $\dict \in \RR^{\psize \times \dsize}$. The linear combination is represented by a vector $\code \in \RR^{\dsize}$ called a code.
The aim is to have a dictionary that is not redundant (independant columns), not too large ($\dsize \ll n$) but that contains the whole picture information.\\
The denoising problem consists then of finding one dictionary for the picture and one sparse code for each patch. Thus it can be reformulated into a minimization problem where we look for the optimal dictionary and codes that are the most similar to each noisy patches:
\begin{equation}
\begin{split}
	& \min_{\dict, \code_i} \sum_{i=1}^{n} ||\code_i||_\pnorm \text{ s.t. } \forall i \in [\![1,n]\!],\ \ ||\noi_i  - \dict \code_i||^2_2 \leqslant \epsilon \\
	& \text{with } \dict \in \RR^{\psize \times \dsize} \text{ and } \forall i \in [\![1,n]\!], \ \code_i \in \RR^\dsize
	\label{sparseDict}
\end{split}
\end{equation}
$\dict \code_i$ is the estimate of the $i^{th}$ denoised patch which should be similar to the noisy patch and $\epsilon$ can be chosen according to the value of the estimated standard deviation of the noise.\\
Once the dictionary and the codes are learned, for each pixel, we have $\psize$ estimations (from the $\psize$ overlapping patches that contain it) and their averaging give us the denoised information of this pixel:
\begin{equation}
	\forall i \in [\![1,n]\!], \ \ \denoi[i] = \frac{1}{\psize} \sum_{j=1}^\psize \dict \code_{p(i,j)}
\end{equation}
Where $p(i,j)$ is the number of the patch that put pixel $\denoi[i]$ in $j^{th}$ position.\\
\\
Equation \eqref{sparseDict} is usualy minimized with $\pnorm = 0$ or $1$. It becomes NP-hard to solve when $\pnorm=0$ but a greedy algorithm such as Orthogonal Recursive Matching Pursuit (ORMP) \cite{OMP} can quickly give a good approximation. The problem is convex with $\pnorm=1$ and is efficiently solved with the Least Angle Regression (LARS) algorithm \cite{LARS}. Experimental observations \cite{l0l1} have shown that the learning part is better with $\pnorm=1$ and the recomposition part with $\pnorm=0$.\\

\subsubsection{Learning Dictionary}

In this part, we consider that $\pnorm = 1$ in equation \eqref{sparseDict}. The problems becomes convex and the equation is only solved in order to find the optimal dictionary $\dict$ that could represents the denoised information of picture $\noi$.\\
The method consists in updating the dictionary iteratively with only a small distribution of $T$ patches. However they are chosen so that they are independently and identically distributed in the picture.\\
First, the initial dictionnary is learned offline on the 10 000 images of the PASCAL VOC'07 database using the  online dictionary learning procedure of \cite{onlineLearning}. This procedure is then used on the noisy picture $\noi$ in order to improve the dictionary efficiency.\\
For each patch $\noi_i$, the corresponding code is computed with the current dictionary kept constant and then the dictionary is updated with all the previous codes.\\
Thus, we minimize iteratively the two equations below:
\begin{itemize}
	\item For a given dictionary $\dict \in \RR^{\psize \times \dsize} \text{ and patch t, } \code^t = \argmin_{\code \in \RR^\dsize} ||\code||_1 \text{ s.t. } ||\noi_t - \dict^{t-1} \code||_2^2 \leq \lambda$
	\item For patches 1 to $t$ and their fixed codes $\code \in \RR^\dsize , \ \ \dict^{t}= \argmin_{\dict \in \RR^{\psize \times \dsize}} \frac{1}{t} \sum_{i=1}^t (\frac{1}{2}|\textbf{\noi}_t^i - \dict \code^i||^2_2 + \lambda ||\code^i||_1)$
\end{itemize}
The first equation is solved with the LARS algorithm.\\
{\color{red} Yohann: TODO}

The second equation can be re-written:
\begin{equation}
	\dict^{t}= \argmin_{\dict \in \RR^{\psize \times \dsize}} \frac{1}{t} (\frac{1}{2} \text{Tr}(\dict^T \dict B^t)- \text{Tr}(D^T C^t )
	\label{BlockDescent}
\end{equation}
where $B$ and $C$ are incrementaly updated with:
\begin{itemize}
	\item $B^t \leftarrow B^{t-1} + \code^t \code^{tT}$
	\item $C^t \leftarrow C^{t-1} + \noi^t \code^{tT}$
\end{itemize}
The dictionary update is thus computed through a block-coordinate descent with warmrestarts \cite{BlockDescent}. This is an iterative method that updates the columns of the dictionary needing only matrices $B$ and $C$ and no matrix inversion in opposite to other approaches as Newton method. Moreover, since the convex optimization problem (Equation \eqref{BlockDescent}) admits separable constraints in the updated blocks (columns), convergence to a global optimum is guaranteed. However Mairal et al empirically found that a single iteration of the dictionary update was sufficient to achieve convergence.

\subsubsection{Denoising}

In this part, we consider that $\pnorm = 0$ in equation \eqref{sparseDict}. The problems becomes NP-hard and the Orthogonal Recursive Matching Pursuit (ORMP) gives a good approximation of the codes for a given dictionary and a given patch. Thus, we find an approximation of the $n$ equations:
\begin{equation}
	\text{For each patch } i, \ \ \code_i = \argmin_{\code} ||\code||_0 \text{ s.t. } ||\noi_i - \dict \code||_2^2 \leq \epsilon
\label{eqORMP}
\end{equation}
If it were perfect, this ORMP would find a patch with the sparsest representation in $\dict$ in which the distance to $\noi_i$ is less than $\epsilon$. In fact the ORMP is not perfect: indeed, it only allows one to find a patch having one sparse (not necessarily the sparsest) representation in $\dict$ and which distance to $\noi_i$ is lower than $\epsilon$.\\
For more explanations on this algorithm, the reader may refer to \cite{KSVD}, in particular the section 2.1.1 Sparse Coding.


\subsection{Simultaneous Sparse Coding}

The idea that developped Mairal et al. in this section is that similar patches should have similar decomposition upon the dictionary. Thus, they created a partition of the picture into $\nclust$ groups of similar patches called clusters. They then added a simultaneous condition which impose similar decompositions for patches of the same cluster and equation \eqref{sparseDict} became:
\begin{equation}
\begin{split}
	&\min_{\dict, \code_{i,j}} \sum_{j=1}^{\nclust} \sum_{i=1}^{n_j} \frac{||\code_{i,j}||^\pnorm_\qnorm}{n_j} \text{ s.t. } \forall j \in [\![1, \nclust]\!],\ \ \sum_{i=1}^{n_j}||\noi_i  - \dict \code_{i,j}||^2_2 \leqslant \epsilon_j \\
	& \text{with } \dict \in \RR^{\psize \times \dsize} \text{ and } \forall i \in [\![1,n_j]\!], \forall j \in [\![1, \nclust]\!], \ \code_{i,j} \in \RR^\dsize
	\label{simultSparseDict}
\end{split}
\end{equation}
where $n_j$ is the size of the $j^{th}$ cluster and $\epsilon_j$ is chosen with respect to the noise inside it.\\
\\
Equation \eqref{simultSparseDict} is usualy minimized with $(\pnorm, \qnorm) = (0, \infty)$ or $(1, 2)$. As for Equation \eqref{sparseDict}, it becomes NP-hard to solve when $(\pnorm, \qnorm)=(0, \infty)$ and Simultaneous Orthogonal Recursive Matching Pursuit (SORMP) \cite{SOMP} gives a good approximation. The problem is convex when $(\pnorm, \qnorm)=(1, 2)$ and a slight modification of the LARS algorithm gives a solution. Once again, the convex problem is used for the learning part while the SORMP is used for the denoising part.\\
As for the non-simultaneous sparse coding, the denoised pixel is approximated by an average of the denoised versions of all the overlapping patches.

\subsubsection{Simultanous Learning}

Not seen in details. The algorithm used is the Simultaneous LARS which is similar to LARS but a "simultaneous" condition that uses cluster information.
{\color{red} \\ Marc:  on va rajouter des trucs l\`a non ?}

\subsubsection{Simultanous Denoising}

Not seen in details. The algorithm used is the Simultaneous ORMP which is similar to ORMP but a "simultaneous" condition that uses cluster information.
{\color{red} \\ Marc:  on va rajouter des trucs l\`a non ?}

%-------------------------------------------------------------------------------
\newpage

\section{Notations}

%-------------------------------------------------------------------------------
\newpage

\section{Algorithm Description}

\subsection{ Algorithm Overview }

The previous parts are used in order to perform a global denoising. First, a rough denoising is realized upon the whole picture. This way, a coherent dictionary is learned and similar patches can be easily regrouped. Then, the clustering is performed and the previous process is applied once more on each cluster indepently. However, the similarity between patches is also used, adding a simultaneous condition upon patches. The whole algorithm is summed up in Algorithm \ref{Global}.\\

\begin{algorithm}[H]
\textbf{Input} : noisy picture $\noi$, pre-learned dictionary $\dict$, number of iterations $N$\\
\textbf{Output} : denoised picture $\denoi$\\
\SetLine
	Update $\dict$ with LARS\\
	Denoise $\noi$ with ORMP\\
	Regroup patches into clusters\\
	\For{ t = 1..N}{
		Update $\dict$ with Simultaneous LARS for each cluster independently\\
		Denoise each cluster with SORMP\\
	}

\caption{Online Dictionary Learning}
\label{Global}
\end{algorithm}
\\
Note that in practice $N=1$ works well and the patchwise update for the dictionary is very costy and do not seem to increase the quality of the denoised picture.\\
The pseudo code is presented below with the same subsections as the theoretical part above. We implemented it in C++ and the link between the pseudo code and the C++ functions is precised each time.

\subsection{Learned Sparse Coding}

\subsubsection{Learning Dictionary}

\begin{itemize}
	\item Main function of the dictionary learning part - \textit{trainL1}: 
\end{itemize}
It computes a distribution of iid patches with the function \textit{getRandList}. It then iteratively minimizes equation \eqref{sparseDict} with either the dictionary $\dict$ fixed or the codes $\code$ fixed. When the dictionary is fixed, the LARS is used in C++ function \textit{computeLars}. When the codes are fixed, the C++ function called is \textit{updateDictionary}.\\

\begin{algorithm}[H]

\SetLine
\textbf{Input} : initial dictionary \dict$^0$ $\in \RR^{\psize \times \dsize}$, number of iterations \textbf{T}, regularization parameter $\lambda$ \\
\textbf{Output} : learned dictionary \dict \\
\textbf{Initialization} : \textbf{A}$^0 \in \RR^{\dsize \times \dsize} \leftarrow 0$,  \textbf{B}$^0 \in \RR^{\psize \times \dsize} \leftarrow 0$ \\
	Compute i.i.d. sampling of \textbf{T} patches of the noisy picture \textbf{Y}$_T$
	\For{ t = 1..\textbf{T}}{
		\textbf{\noi}$_t$ = \textbf{\Noi}$_T$[t]\\
		Sparse coding: compute with LARS algorithm:
\[
		\textbf{$\code$}^t = \argmin_{\code \in \RR^\dsize} ||\code||_1 \text{ s.t. } ||\noi_t - \dict^{t-1} \code||_2^2 \leq \lambda
\]
		\textbf{A}$^t \leftarrow \textbf{A}^{t-1} + \code^t \code^{t T}$\\
		\textbf{B}$^t \leftarrow \textbf{B}^{t-1} + \textbf{\noi}_t^t \code^{t T}$\\
		Update dictionary from \dict$^{t-1}$ to \dict$^{t}$ so that:
\[
		\dict^{t}= \argmin_{\dict \in\RR^{\psize \times \dsize}} \frac{1}{t} \sum_{i=1}^t (\frac{1}{2}|\textbf{\noi}_t^i - \dict \code^i||^2_2 + \lambda ||\code^i||_1)
\]
}
\Return \dict$^{T}$

\caption{\textit{trainL1}}

\end{algorithm}

\begin{itemize}
	\item Dictionary update - \textit{updateDictionary}: 
\end{itemize}
Block-coordinate descent with warmrestarts algorithm that minimizes Equation \eqref{BlockDescent}.\\
{\color{red}TODO: se renseigner sur la  methode ? pk si A(j,j) = 0 ...} \\

\begin{algorithm}[H]

\SetLine
\textbf{Input} : input dictionary \dict = [\textbf{d}$^1$, ... , \textbf{d}$^\dsize$] $\in \mathbb{R}^{\psize \times \dsize}$, \\ \textbf{A} = [\textbf{a}$^1$, ... , \textbf{a}$^\dsize$] $ \in \mathbb{R}^{\dsize \times \dsize}$,  \textbf{B} = [\textbf{b}$^1$, ... , \textbf{b}$^\dsize$]$ \in \mathbb{R}^{\psize \times \dsize}$\\
\textbf{Output} : updated dictionary \textbf{\dict} \\
\For{ j = 1..\dsize}{
	Update the $j^{th}$ column:\\
\If{ \textbf{A}(j,j) = 0 }{
\[
		\textbf{d}^{j}\leftarrow 0
\]
}
\Else{
\[
		\textbf{u}^{j}\leftarrow \frac{1}{\textbf{A}(j,j)} (\textbf{b}^{j} -\dict\textbf{a}^{j}) + \textbf{d}^{j}
\]
\[
		\textbf{d}^{j}\leftarrow \frac{1}{\max(||\textbf{u}^{j}||_2, 1)}\textbf{u}^{j}
\]
}
}

\Return \dict

\caption{\textit{updateDictionary}}

\end{algorithm}

\newpage

\begin{itemize}
	\item Least Angle Regression (LARS) - \textit{computeLars}: 
\end{itemize}
The LARS algorithm is very long and has been split up for better readibility. \\
($\min^+$ is the minimum between strictly positive values only)\\
{\color{red}TODO: preciser la version + pk inversion bizarre des matrices}
\\

\begin{algorithm}[H]

\SetLine
\textbf{Input} : Input dictionary $\dict \in \mathbb{R}^{\psize \times \dsize}$, noisy patch \textbf{\noi} $\in \mathbb{R}^{\psize}$, constraint $\lambda \in \mathbb{R}$\\
\textbf{Output} : code $\code \in \mathbb{R}^{\dsize}$\\
\vspace{1mm}

\textbf{Initialization:}\\
Initialize variables\\
\vspace{2mm}

\For{ i = 1..$\dsize$}{
	\vspace{1mm}

	\textbf{New atom:}\\
	Add a new atom in $\code$ if there is no downdating\\
	\vspace{2mm}

	\textbf{Compute steps:}\\
	Compute the initial step ($\gamma$)\\
	Compute the critical step that impose downdating (stepDownDate)\\
	Compute the critical step that breaks the loop and ends the algorithm (stepMax)\\
	\vspace{2mm}

	\textbf{Update code:}\\
	Compute the new code $\code$ and downdate or break if needed
	\vspace{2mm}

}
\Return $\code$
\vspace{2mm}

\caption{\textit{computeLars} - Main}

\end{algorithm}

\begin{algorithm}[H]

\textbf{Compute Gram matrix:}\\
\vspace{0.5mm}
G $\in \mathbb{R}^{\dsize \times \dsize} \leftarrow \dict^T\dict$\\
\vspace{2mm}

\textbf{Initialize patch norm} and check for trivial solution:\\
\vspace{0.5mm}
normPatch $\in \mathbb{R}^+ \leftarrow$ $||\noi||^2_2$\\
\If{ normPatch $< \lambda$ }{
	\textbf{return 0}
}
\vspace{2mm}

\textbf{Initialize code and active index:}\\
\vspace{0.5mm}
$\code \in \mathbb{R}^{\dsize} \leftarrow$ \textbf{0}\\
$\A \in [\![0, \dsize]\!]^{\dsize} \leftarrow$ \textbf{0}\\
\vspace{2mm}

\textbf{Compute most correlated element:}\\
\vspace{0.5mm}
$\clars \in \RR^{\dsize} \leftarrow \dict^T \noi$\\
$C \in \RR^+ \leftarrow \max_{j=1..\dsize}(|\clars[j]|)$ \\
currentInd $\leftarrow j \text{ s.t. } \clars[j] = C$\\
\vspace{2mm}

\textbf{Add a new atom at first iteration:}\\
\vspace{0.5mm}
newAtom $\leftarrow$ \textbf{True}\\
\vspace{2mm}

\caption{\textit{computeLars} - Initialization}
\end{algorithm}

\newpage

\noindent At each iteration, the code $\code$ can be updated. If it is, a new atom is added to it making it less sparse but a better approximation of the patch. The atom added is the new most correlated element. Though adding a new element to the code is not very costful, the Gram matrices have to be updated of one line and one inverse has to be calculated. One could theorically computes the inverse with any algorithm but the LARS algorithm is iterated a lot in the whole algorithm and the inversion has to be very fast in order to have a not too slow denoising method. Thus, the inverse is computed iteratively, using the previous inverse to compute it quickly. {\color{red} precise method name.}\\

\begin{algorithm}[H]
\SetLine

\If{ newAtom }{
\vspace{0.5mm}
	$\A[i] \leftarrow $ currentInd\\
	$G_A \in \mathbb{R}^{\dsize \times i}, G_S \in \mathbb{R}^{i \times i}$\\
	$G_A$[$i^{th}$ column] $\leftarrow$ $G$[currentInd$^{th}$ column]\\
	$G_S$[$i^{th}$ line] $\leftarrow$ $G_A$[currentInd$^{th}$ line]\\
	symmetrize $G_S$: copy the lower part of $G_S$ into its upper part\\
	UPDATE $G_S^{-1}$
}

\caption{\textit{computeLars} - New atom}
\end{algorithm}\\

\noindent The algorithm computes 3 steps at each iteration:
\begin{itemize}
	\item $\gamma$ is the step that allows to reach the new most correlated element. With this step, all the active indexes and the most correlated of the non-active indexes will have the same correlation. Thus, this index will be added to the code and become active.
	\item stepDownDate is a limit for $\gamma$. If it is reached, one active-index (downDateInd) ... {\color{red} precise !!!}
	\item stepMax is also a limit for $\gamma$. When reached, the algorithm stops because the best solution for Equation \eqref{}has been found.{\color{red} link to LARS equation !!!}
\end{itemize}

\begin{algorithm}[H]
\textbf{Initial step:}\\
\vspace{0.5mm}
u $\in \mathbb{R}^i \leftarrow$ $G_S^{-1} (\text{sgn}(\clars[\A[j]]))_{j\in [\![1, i]\!]}$\\
$C \leftarrow |\clars[\A[1]]| = \max_{j=1..\dsize}(|\clars[j]|)$\\
$\gamma \in \mathbb{R}^+_* \leftarrow$ $\min^+\left(\frac{C + \clars[j]}{1 + (G_Au)[j]}, \frac{C - \clars[j]}{1 - (G_Au)[j]}\right)_{j \text{ s.t. } \A[j] = 0}$\\
currentInd $=j$ s.t. $\gamma = \frac{C \pm \clars[j]}{1 \pm (G_Au)[j]}$\\
\vspace{2mm}

\textbf{Downdate step:}\\
\vspace{0.5mm}
ratio $\in \mathbb{R}^i \leftarrow$ $\left(-\frac{\code[\A[j]]}{u_j}\right)_{j\in[\![1, i]\!]}$\\
stepDownDate $\in \mathbb{R}^+_* \leftarrow$ $\min^+(\text{ratio})$ \\
downDateInd $\in [\![1, \dsize]\!] \leftarrow \A[j]$ s.t. ratio[j] =	stepDownDate\\
\vspace{2mm}

\textbf{Breaking step:}\\
\vspace{0.5mm}
$a \in \mathbb{R} \leftarrow$ $\sum_{j \in [\![1, i]\!]} \text{sgn}(\clars[\A[j]])u[j]$\\
$b \in \mathbb{R} \leftarrow$ $\sum_{j \in [\![1, i]\!]} \clars[\A[j]] u[j]$\\
$c \in \mathbb{R} \leftarrow$ normPatch - $\lambda$\\
$\Delta \in \mathbb{R} \leftarrow$ $b^2 - ac$ \\
stepMax $\in \mathbb{R}^+ \leftarrow$ $\min(\frac{b - \sqrt{\Delta}}{a}, C)$ \\
\vspace{2mm}

\caption{\textit{computeLars} - Compute steps}
\end{algorithm}

\newpage

\noindent The final step used is the minimum of the 3 steps computed. Once it is chosen, the algorithm has 3 different ways to go on:
\begin{itemize}
	\item $\gamma$: the code is updated and a new atom will be added to it. The loop index is incremented.
	\item stepDownDate: the code is downdated, one atom has to be deleted. The loop index is decremented.
	\item stepMax: the optimal solution has been found, the loop breaks and the LARS is over.
\end{itemize}

\begin{algorithm}[H]

\SetLine
$\gamma$ $\leftarrow$ min($\gamma$, stepDownDate, stepMax)\\
\For{ j = 1..i}{
	$\code[\A[j]] \leftarrow \code[\A[j]] + \gamma$u[j]\\
}	
$\clars$ $\leftarrow$ $\clars - \gamma G_Au$ \\
normPatch $\leftarrow$ normPatch + $a\gamma^2 - 2b\gamma$ \\
\If{ $|\gamma| < 10^{-6} $ {\bf or} $\gamma = stepMax$ {\bf or} $normPatch  < 10^{-6}$ {\bf or} $normPatch - \lambda < 10^{-6}$}{
	\textbf{break}
}
\If{ $\gamma = stepDownDate$ }{
	DOWNDATE $G_S^{-1}$ w.r.t downDateInd\\
	$\A[\text{downDateInd}] \leftarrow \ 0$\\
	$\code$[downDateInd] $\leftarrow$ 0\\
	newAtom $\leftarrow$ \textbf{False} \\
	i $\leftarrow$ i - 1
}
\Else{
	newAtom $\leftarrow$ \textbf{True} \\
	i $\leftarrow$ i + 1
}
	
\caption{\textit{computeLars} - Update code}

\end{algorithm}

\newpage

\begin{algorithm}[H]

\SetLine
\textbf{Input} : Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its former inverse to update $G_S^{-1}\in \mathbb{R}^{i-1 \times i-1}$\\
\textbf{Output} : updated  $G_S^{-1}\in \mathbb{R}^{i \times i}$\\
\If{i = 1}{
	\Return $\frac{1}{G_S}$
} 
$u \ \leftarrow \ G_S^{-1} G_S[i^{th}$ line]\\
$\sigma$ $\leftarrow \ \frac{1}{G_s(i,i) - u.G_S[i^{th} \text{ line}]}$\\
$G_s^{-1}(i,i)\ \leftarrow$ $\sigma$\\
$G_s^{-1}[i^{th}$ line] $\leftarrow$ -$\sigma u$\\
\Return $G_s^{-1} \ \leftarrow \ G_s^{-1}+\sigma u u^T$\\
\caption{\textit{updateGram} {\color{blue} Marc: Algo correspondant, mais \`a valider.}}

\end{algorithm}

\begin{algorithm}[H]

\SetLine
\textbf{Input} : pseudo-Gram matrix $G_A \in \mathbb{R}^{\dsize \times i}$
Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its 
inverse $G_S^{-1}\in \mathbb{R}^{i \times i}$, criticalInd $\in [\![1, \dsize]\!]$, current iteration $i$\\
\textbf{Output} : downdated matrices $G_A \in \mathbb{R}^{\dsize \times i-1}, G_S, G_S^{-1} \in \mathbb{R}^{i-1 \times i-1}$\\

$\sigma \ \leftarrow \ \frac{1}{G_S^{-1}(\text{criticalInd}, \text{criticalInd})}$\\
$u \ \leftarrow \ G_S^{-1} [$criticalInd$^{th}$ line] without its criticalInd$^{th}$ coefficient\\
\For{j= criticalInd:i-1}{
	$G_A[j^{th}$ column] $\leftarrow$ $G_A[(j+1)^{th}$ column]\\
	\For{k= 1:criticalInd-1}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k)$\\
	}
	\For{k= criticalInd:i}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k+1)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k+1)$\\
	}
}
$G_s^{-1} \ \leftarrow \ G_s^{-1}-\sigma u u^T$\\

\caption{\textit{downdateGram} {\color{blue} Marc: Algo correspondant, mais \`a valider.}}

\end{algorithm}

%-------------------------------------------------------------------------------
\section*{Glossary}



%-------------------------------------------------------------------------------
\section*{Image Credits}

\includegraphics[height=2em]{ipol_logo} \copyright\ IPOL (there's no
need to credit this image, here is used as an example.)

%-------------------------------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{Patch01}
	A. Efros, and T. Leung
	\emph{Texture synthesis by non-parametric sampling}.
	ICCV,
	1999.

\bibitem{Patch02}
	A. Buades, B. Coll, and J. Morel
	\emph{A non-local algorithm for image denoising}.
	CVPR,
	2005.

\bibitem{LARS}
	B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani
	\emph{Least angle regression}. 
	Ann. Statist.,
	32(2):407–499,
	2004.

\bibitem{onlineLearning}
	J.Mairal, F. Bach, J. Ponce, and G. Sapiro
	\emph{Online dictionary learning for sparse coding}.
	ICML,
	2009.

\bibitem{LSSC}
	J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman
	\emph{ Non-local Sparse Models for Image Restoration}.
	International Conference on Computer Vision,
	2009.

\bibitem{thesis}
	J. Mairal
	\emph{ Representations parcimonieuses en apprentissage statistique, traitement d’image et
vision par ordinateur}.
	PhD thesis,
	2010.

\bibitem{OMP}
	S. Mallat and Z. Zhang
 	\emph{Matching pursuit in a timefrequency dictionary}.
	IEEE T. SP,
	41(12):3397–3415,
	1993.

\bibitem{l0l1}
	M. Elad and M. Aharon
	\emph{Image denoising via sparse and redundant representations over learned dictionaries}.
	IEEE T.
	IP, 54(12):3736–3745,
	2006.
	
\bibitem{BlockDescent}	
	D. P. Bertsekas
	\emph{Nonlinear programming}.
	Athena Scientific Belmont,
	1999.
	
\bibitem{SOMP}
	J. A. Tropp
	\emph{Algorithms for simultaneous sparse approximation}.
	Sig. Proc.,
	86:572–602,
	2006.
	
\bibitem{KSVD}
	M. Lebrun and A. Leclaire
	\emph{{An Implementation and Detailed Analysis of the K-SVD Image Denoising Algorithm}}.
	{{Image Processing On Line}}.,
	2012.

\end{thebibliography}


\end{document}
%-------------------------------------------------------------------------------
