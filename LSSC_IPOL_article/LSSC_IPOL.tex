%-------------------------------------------------------------------------------
% IPOL article about Non-local Sparse Models for Image Restoration
% by Yohann Salaun & Marc Lebrun
% February 2013
%-------------------------------------------------------------------------------
\documentclass{ipol}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{algorithm2e}
\usepackage{bm}
\usepackage{verbatim}

% commands for dictionary/patches...
\newcommand{\psize}{m}   
\newcommand{\dsize}{k}  
\newcommand{\dict}{\textbf{D}}  
\newcommand{\code}{\bm{\alpha}}  
\newcommand{\pnorm}{p}
% commands for noisy/denoised picture
\newcommand{\Denoi}{\textbf{X}}
\newcommand{\denoi}{\textbf{x}}
\newcommand{\Noi}{\textbf{Y}}
\newcommand{\noi}{\textbf{y}}
% commands for LARS
\newcommand{\mua}{\bm{\mu}_{\mathcal{A}}}
\newcommand{\da}{\bm{D}_{\mathcal{A}}}
\newcommand{\Aa}{A_{\mathcal{A}}}
\newcommand{\una}{\mathds{1}_{\mathcal{A}}}
\newcommand{\ua}{\bm{u}_{\mathcal{A}}}
\newcommand{\clars}{\hat{\textbf{c}}}
\newcommand{\A}{\mathcal{A}}
% usual commands
\newcommand{\RR}{\mathbb{R}}
\newcommand{\Rn}{\mathbb{R}^n}
\newcommand{\argmin}{\text{argmin}}

\ipolSetTitle{LSSC}
\ipolSetAuthors{Yohann Salaun$^1$ \& Marc Lebrun$^2$}
\ipolSetAffiliations{%
$^1$ Polytechnique, France (\texttt{yohann.salaun@polytechnique.org})\\
$^2$ CMLA, ENS Cachan, France (\texttt{marc.lebrun@cmla.ens-cachan.fr})}

\begin{document}

%-------------------------------------------------------------------------------
\begin{abstract}

\end{abstract} 

%-------------------------------------------------------------------------------
\section{Overview}


%-------------------------------------------------------------------------------
\section{Theoritical Description}

\subsection{ Notations}

In order to keep coherence with \cite{LSSC}, the notations used are the same.
A picture of $n$ pixels is seen as a column vector in $\Rn$. The noisy picture is noted $\noi$ and the denoised one $\denoi$. The $i$-th pixel of x is noted $\denoi[i]$ and the patch centered in $\denoi[i]$ and of size $\psize$ is noted $\denoi_i$.

\subsection{ Patches }
TODO: change name

The first idea behind this method is to decompose the noisy picture into patches of equal size. \\
TODO: developp patch idea + the fact that there is one patch per pixel\\

\subsection{ Learned Sparse Coding}

The second idea is to assume that the denoised patches can be approximated by a sparse linear combinations of elements from a basis set that someway sums up the information belonging to the picture.\\
The basis set is called a dictionary $\dict \in \RR^{\psize \times \dsize}$ and is composed of $\dsize$ elements. The linear combination is represented by a vector $\code \in \RR^{\dsize}$ called a code.\\
TODO: formulation is hard....
The denoising problem is then reformulated into a minimization problem where we look for the optimal dictionary and codes solving the equation below:
\begin{equation}
	min_{\code\in \RR^\dsize, \dict \in \RR^{\psize \times \dsize}} \sum_{i=1}^{n} ||\code_i||_\pnorm \text{ s.t. } ||\noi_i  - \dict \code_i||^2_2 \leqslant \epsilon
	\label{sparseDict}
\end{equation}
$\dict \code$ is the estimate of the denoised patch which should be similar to the noisy patch and $\epsilon$ can be chosen according to the value of the estimated standard deviation of the noise.
Once the dictionary and the codes are learned, for each pixel, we have $\psize$ estimations (from the $\psize$ patches that contain it) and their averaging give us the denoised information of this pixel:
\begin{equation}
	\forall i \in [1,n], \ \ \denoi[i] = \frac{1}{\psize} \sum_{j=1}^\psize \dict \code_{\sigma(i,j)}
\end{equation}
Where $\sigma(i,j)$ is the number of the patch where pixel $\denoi[i]$ is in $j^{th}$ position.\\
\\
\eqref{sparseDict} is usualy minimized with $\pnorm = 0$ or $1$. It becomes NP-hard to solve when $\pnorm=0$ but a greedy algorithm such as Orthogonal Matching Pursuit \cite{OMP} can quickly give a good approximation. The problem is convex with $\pnorm=1$ and is efficiently solved with the Least Angle Regression algorithm \cite{LARS}. Experimental observations \cite{l0l1} have shown that the learning part is better with $\pnorm=1$ and the recomposition part with $\pnorm=0$.\\

\subsubsection{Least Angle Regression}

TODO: explain the subject briefly and how to solve it
\begin{itemize}
	\item For $\dict \in \RR^{\psize \times \dsize}, \ \ \code^t = \argmin_{\code \in \RR^\dsize} ||\code||_1 \text{ s.t. } ||\noi_t - \dict^{t-1} \code||_2^2 \leq \lambda$
	\item For $\code \in \RR^\dsize, \ \ \dict^{t}= \argmin_{\dict \in \RR^{\psize \times \dsize}} \frac{1}{t} \sum_{i=1}^t (\frac{1}{2}|\textbf{\noi}_t^i - \dict \code^i||^2_2 + \lambda ||\code^i||_1)$
\end{itemize}

\subsubsection{Orthogonal Matching Pursuit}
TODO: MARC :)

\subsection {Simultaneous Sparse Coding}


%-------------------------------------------------------------------------------
\section{Algorithm Description}

\subsection{ Algorithm Overview }

The first part consists in initializing a dictionary that will denoise roughly the picture.\\
Once the picture is denoised a first time, a clustering is made in order to regroup similar patches for further treatment.\\
Then, iteratively for each clusters, the dictionary is updated using simultaneous sparse coding and the cluster is denoised.

\subsection{Dictionnary Initialization}

The initial dictionnary is first learned offline on the 10 000 images of the PASCAL VOC'07 database using the  online dictionary learning procedure of \cite{onlineLearning}. This procedure is then used on the noisy picture in order to improve the dictionary efficiency.\\
In fact, only a fixed number $T$ of patches in the picture are used to update the dictionary. However they are chosen so that they are independently and identically distributed in the picture.\\
The algorithm corresponds then to the minimization of eq.\ref{sparseDict} on the $T$ patches with the $l_1$ norm using the LARS \cite{LARS} algorithm.\\

\begin{algorithm}[H]

\SetLine
\textbf{Input} : number of iterations \textbf{T},  i.i.d. sampling of \textbf{T} patches of the noisy picture \textbf{Y}$_T$, initial dictionary \dict$^0$ $\in \RR^{\psize \times \dsize}$, regularization parameter $\lambda$ \\
\textbf{Output} : learned dictionary \dict \\
\textbf{Initialization} : \textbf{A}$^0 \in \RR^{\dsize \times \dsize} \leftarrow 0$,  \textbf{B}$^0 \in \RR^{\psize \times \dsize} \leftarrow 0$ \\
	\For{ t = 1..\textbf{T}}{
		\textbf{\noi}$_t$ = \textbf{\Noi}$_T$[t]\\
		Sparse coding: compute with LARS algorithm:
\[
		\textbf{$\code$}^t = \argmin_{\code \in \RR^\dsize} ||\code||_1 \text{ s.t. } ||\noi_t - \dict^{t-1} \code||_2^2 \leq \lambda
\]
		\textbf{A}$^t \leftarrow \textbf{A}^{t-1} + \code^t \code^{t T}$\\
		\textbf{B}$^t \leftarrow \textbf{B}^{t-1} + \textbf{\noi}_t^t \code^{t T}$\\
		Update dictionary from \dict$^{t-1}$ to \dict$^{t}$ so that:
\[
		\dict^{t}= \argmin_{\dict \in\RR^{\psize \times \dsize}} \frac{1}{t} \sum_{i=1}^t (\frac{1}{2}|\textbf{\noi}_t^i - \dict \code^i||^2_2 + \lambda ||\code^i||_1)
\]
}
\Return \dict$^{T}$

\caption{Online Dictionary Learning}

\end{algorithm}

\begin{algorithm}[H]

\SetLine
\textbf{Input} :input dictionary \dict = [\textbf{d}$^1$, ... , \textbf{d}$^\dsize$] $\in \mathbb{R}^{\psize \times \dsize}$, \\ \textbf{A} = [\textbf{a}$^1$, ... , \textbf{a}$^\dsize$] $ \in \mathbb{R}^{\dsize \times \dsize}$,  \textbf{B} = [\textbf{b}$^1$, ... , \textbf{b}$^\dsize$]$ \in \mathbb{R}^{\psize \times \dsize}$\\
\textbf{Output} : updated dictionary \textbf{\dict} \\
\Repeat{convergence {\color{red} Marc: apparemment, dans le code on a une boucle sur params.updateIteration = 1. Est-ce qu'il ne vaudrait mieux pas calculer l'argmin \`a chaque boucle et s'arr\^eter lorsque c'est plus petit qu'une certaine valeur ?}{\color{green} Yohann: je n'ai pas trop compris ce que tu voulais faire. Ca c'est le pseudo code présenté par Mairal, qui est donc une sorte de descente de gradient. Ce genre d'algo converge en théorie à l'infini, en pratique on prend un grand nombre d'itération. Cependant, dans ce cas on fait une minimisation alternée d'un problème plus global (selon D puis selon alpha et on itère). Du coup, et surement à cause de contraintes de temps, Mairal a fixé le nombre d'itération à 1 mais permet de le changer en paramètre dans son algo. Du coup j'ai voulu faire pareil que Mairal. }}{
	\For{ j = 1..\dsize}{
		update the $j^{th}$ column:\\
	\If{ \textbf{A}(j,j) = 0 }{
\[
		\textbf{d}^{j}\leftarrow 0
\]
	}
	\Else{
\[
		\textbf{u}^{j}\leftarrow \frac{1}{\textbf{A}(j,j)} (\textbf{b}^{j} -\dict\textbf{a}^{j}) + \textbf{d}^{j}
\]
\[
		\textbf{d}^{j}\leftarrow \frac{1}{\max(||\textbf{u}^{j}||_2, 1)}\textbf{u}^{j}
\]
	}
	}
}

\Return \dict

\caption{Dictionary Update {\color{red} updateDictionary} {\color{blue} Marc: Algo valid\'e}}

\end{algorithm}

\newpage

\begin{algorithm}[H]

\SetLine
{\color{red} Marc: TODO : choisir entre les indices ou les crochets pour les indices\\}
\textbf{Input} : Input dictionary $\dict \in \mathbb{R}^{\psize \times \dsize}$, noisy patch \textbf{\noi} $\in \mathbb{R}^{\psize}$, constraint $\lambda \in \mathbb{R}$\\
\textbf{Output} : code $\code \in \mathbb{R}^{\dsize}$\\
---INITIALIZATION---\\
G $\in \mathbb{R}^{\dsize \times \dsize} \leftarrow \dict^T\dict$\\
normPatch $\in \mathbb{R}^+ \leftarrow$ $||\noi||^2_2$\\
$\code \in \mathbb{R}^{\dsize} \leftarrow$ \textbf{0}\\
$\A \in [0;\dsize]^{\dsize} \leftarrow$ \textbf{0}\\
\textit{Most correlated element}\\
$\clars \in \RR^{\dsize} \leftarrow \dict^T \noi$\\
$C \in \RR^+ \leftarrow \max_{j=1..\dsize}(|\clars_j|)$ \\
currentInd $\leftarrow j \text{ s.t. } \clars_j = C$\\
newAtom $\leftarrow$ \textbf{True}\\
\If{ normPatch $< \lambda$ }{
	\textbf{return 0}
}
\For{ i = 1..$\dsize$}{
LOOP, see below
}
\Return $\code$
	
\caption{LARS algorithm - Mairal Version {\color{red} computeLars}}

\end{algorithm}

\newpage

\begin{algorithm}[H]

\SetLine
{\color{red} Marc: TODO : choisir entre les indices ou les crochets pour les indices\\}
	---NEW ATOM---\\
	\If{ newAtom }{
		$\A[i] \leftarrow $ currentInd\\
		$G_A \in \mathbb{R}^{\dsize \times i}, G_S \in \mathbb{R}^{i \times i}$\\
		$G_A$[$i^{th}$ column] $\leftarrow$ $G$[currentInd$^{th}$ column]\\
		$G_S$[$i^{th}$ line] $\leftarrow$ $G_A$[currentInd$^{th}$ line]\\
		symmetrize $G_S$\\
		UPDATE $G_S^{-1}$
	}
	---VARIABLES UPDATES---\\
	u $\in \mathbb{R}^i \leftarrow$ $G_S^{-1} (\text{sgn}(\clars_{\A[j]}))_{j\in [1;i]}$\\
	$C \leftarrow |\clars[\A[1]]| = \max_{j=1..\dsize}(|\clars_j|)$\\
	$\gamma \in \mathbb{R}^+_* \leftarrow$ $\min^+\left(\frac{C 
	+ \clars_j}{1 + (G_Au)[j]}, \frac{C - \clars_j}{1 - (G_Au)[j]}\right)_{j \text{ s.t. } \A[j] = 0}$\\
	currentInd $=j$ s.t. $\gamma = \frac{C \pm \clars_j}{1 \pm (G_Au)[j]}$\\
	ratio $\in \mathbb{R}^i \leftarrow$ $\left(-\frac{\code[\A[j]]}{u_j}\right)_{j\in[1;i]}$\\
	stepDownDate $\in \mathbb{R}^+_* \leftarrow$ $\min^+(\text{ratio})$ ($\min^+$ is the minimum between strictly positive values only)\\
	downDateInd $\in [1;\dsize] \leftarrow \A[j]$ s.t. ratio[j] =	stepDownDate\\
	---POLYNOMIAL RESOLUTION---\\
	$a \in \mathbb{R} \leftarrow$ $\sum_{j \in [1;i]} \text{sgn}(\clars[\A[j]])u[j]$\\
	$b \in \mathbb{R} \leftarrow$ $\sum_{j \in [1;i]} \clars[\A[j]] u[j]$\\
	$c \in \mathbb{R} \leftarrow$ normPatch - $\lambda$\\
	$\Delta \in \mathbb{R} \leftarrow$ $b^2 - ac$ \\
	stepMAX $\in \mathbb{R}^+ \leftarrow$ $\min(\frac{b - \sqrt{\Delta}}{a}, C)$ \\
	---FINAL STEP $\&$ BREAK---\\
	$\gamma$ $\leftarrow$ min($\gamma$, stepDownDate, stepMAX)\\
	\For{ j = 1..i}{
		$\code[\A[j]] \leftarrow \code[\A[j]] + \gamma$u[j]\\
	}	
	$\clars$ $\leftarrow$ $\clars - \gamma G_Au$ \\
	normPatch $\leftarrow$ normPatch + $a\gamma^2 - 2b\gamma$ \\
	\If{ $|\gamma| < 10^{-6} $ {\bf or} $\gamma = stepMAX$ {\bf or} $normPatch  < 10^{-6}$ {\bf or} $normPatch - \lambda < 10^{-6}$}{
		\textbf{break}
	}
	\If{ $\gamma = stepDownDate$ }{
		DOWNDATE $G_S^{-1}$ w.r.t downDateInd\\
		$\A[\text{downDateInd}] \leftarrow \ 0$\\
		$\code$[downDateInd] $\leftarrow$ 0\\
		newAtom $\leftarrow$ \textbf{False} \\
		i $\leftarrow$ i - 1
	}
	\Else{
		newAtom $\leftarrow$ \textbf{True} \\
		i $\leftarrow$ i + 1
	}
	
\caption{LARS algorithm - Mairal Version {\color{red} computeLars {\color{blue} Marc: Algo correspondant, mais \`a valider.}} - LOOP }

\end{algorithm}
\newpage

\begin{algorithm}[H]

\SetLine
\textbf{Input} : Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its former inverse to update $G_S^{-1}\in \mathbb{R}^{i-1 \times i-1}$\\
\textbf{Output} : updated  $G_S^{-1}\in \mathbb{R}^{i \times i}$\\
\If{i = 1}{
	\Return $\frac{1}{G_S}$
} 
$u \ \leftarrow \ G_S^{-1} G_S[i^{th}$ line]\\
$\sigma$ $\leftarrow \ \frac{1}{G_s(i,i) - u.G_S[i^{th} \text{ line}]}$\\
$G_s^{-1}(i,i)\ \leftarrow$ $\sigma$\\
$G_s^{-1}[i^{th}$ line] $\leftarrow$ -$\sigma u$\\
\Return $G_s^{-1} \ \leftarrow \ G_s^{-1}+\sigma u u^T$\\
\caption{Update invert algorithm {\color{red} updateGram} {\color{blue} Marc: Algo correspondant, mais \`a valider.}}

\end{algorithm}

\begin{algorithm}[H]

\SetLine
\textbf{Input} : pseudo-Gram matrix $G_A \in \mathbb{R}^{\dsize \times i}$
Gram matrix $G_S \in \mathbb{R}^{i \times i}$, and its 
inverse $G_S^{-1}\in \mathbb{R}^{i \times i}$, criticalInd $\in [1; \dsize]$, current iteration $i$\\
\textbf{Output} : downdated matrices $G_A \in \mathbb{R}^{\dsize \times i-1}, G_S, G_S^{-1} \in \mathbb{R}^{i-1 \times i-1}$\\

$\sigma \ \leftarrow \ \frac{1}{G_S^{-1}(\text{criticalInd}, \text{criticalInd})}$\\
$u \ \leftarrow \ G_S^{-1} [$criticalInd$^{th}$ line] without its criticalInd$^{th}$ coefficient\\
\For{j= criticalInd:i-1}{
	$G_A[j^{th}$ column] $\leftarrow$ $G_A[(j+1)^{th}$ column]\\
	\For{k= 1:criticalInd-1}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k)$\\
	}
	\For{k= criticalInd:i}{
		$G_S(j,k)$ $\leftarrow$ $G_S(j+1,k+1)$\\
		$G_S^{-1}(j,k)$ $\leftarrow$ $G_S^{-1}(j+1,k+1)$\\
	}
}
$G_s^{-1} \ \leftarrow \ G_s^{-1}-\sigma u u^T$\\

\caption{Downdate invert algorithm {\color{red} downdateGram} {\color{blue} Marc: Algo correspondant, mais \`a valider.}}

\end{algorithm}

%-------------------------------------------------------------------------------
\section*{Glossary}



%-------------------------------------------------------------------------------
\section*{Image Credits}

\includegraphics[height=2em]{ipol_logo} \copyright\ IPOL (there's no
need to credit this image, here is used as an example.)

%-------------------------------------------------------------------------------
\section{References}

\begin{thebibliography}{9}

\bibitem{LARS}
	B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani
	\emph{Least angle regression}. 
	Ann. Statist.,
	32(2):407–499,
	2004.

\bibitem{onlineLearning}
	J.Mairal, F. Bach, J. Ponce, and G. Sapiro
	\emph{Online dictionary learning for sparse coding}.
	ICML,
	2009.

\bibitem{LSSC}
	J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman
	\emph{ Non-local Sparse Models for Image Restoration}.
	International Conference on Computer Vision,
	2009.

\bibitem{thesis}
	J. Mairal
	\emph{ Représentations parcimonieuses en apprentissage statistique, traitement d’image et
vision par ordinateur}.
	PhD thesis,
	2010.

\bibitem{OMP}
	S. Mallat and Z. Zhang
 	\emph{Matching pursuit in a timefrequency dictionary}.
	IEEE T. SP,
	41(12):3397–3415,
	1993.

\bibitem{l0l1}
	M. Elad and M. Aharon
	\emph{Image denoising via sparse and redundant representations over learned dictionaries}.
	IEEE T.
	IP, 54(12):3736–3745,
	2006.

\end{thebibliography}


\end{document}
%-------------------------------------------------------------------------------
